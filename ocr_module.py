#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Thai OCR with PDF Support
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á PDF ‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û
"""

import cv2
import numpy as np
import pytesseract
from PIL import Image
import re
import os

# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDF
try:
    import PyPDF2
    import fitz  # PyMuPDF
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False
    print("‚ö†Ô∏è  PDF libraries not installed. Install with:")
    print("   pip install PyPDF2 PyMuPDF")


class EnhancedThaiDocumentOCR:
    def __init__(self):
        self.thai_digits = {
            '0': '‡πê', '1': '‡πë', '2': '‡πí', '3': '‡πì', '4': '‡πî',
            '5': '‡πï', '6': '‡πñ', '7': '‡πó', '8': '‡πò', '9': '‡πô'
        }

    # ============================================================
    # üîπ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö PDF ‡∏°‡∏µ text layer ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # ============================================================
    def check_pdf_has_text(self, pdf_path):
        if not PDF_SUPPORT:
            return False

        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                pages_to_check = min(3, len(pdf_reader.pages))
                for i in range(pages_to_check):
                    page = pdf_reader.pages[i]
                    text = page.extract_text()
                    thai_chars = len(re.findall(r'[‡∏Å-‡∏Æ‡∏∞-‡πå]', text))
                    if thai_chars > 50:
                        return True
            return False
        except Exception as e:
            print(f"Error checking PDF: {e}")
            return False

    # ============================================================
    # üîπ Extract text ‡∏à‡∏≤‡∏Å PDF ‡∏ó‡∏µ‡πà‡∏°‡∏µ text layer
    # ============================================================
    def extract_text_from_pdf(self, pdf_path):
        if not PDF_SUPPORT:
            raise ImportError("PyPDF2 not installed")

        print("‚úì PDF has text layer - extracting directly...")
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                all_text = []
                for i, page in enumerate(pdf_reader.pages):
                    print(f"  Page {i+1}/{len(pdf_reader.pages)}")
                    text = page.extract_text()
                    all_text.append(text)
                return '\n\n'.join(all_text)
        except Exception as e:
            print(f"‚úó Error extracting text: {e}")
            return None

    # ============================================================
    # üîπ ‡πÅ‡∏õ‡∏•‡∏á PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏û (‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ text layer)
    # ============================================================
    def pdf_to_images(self, pdf_path, output_folder='temp_pages', dpi=300):
        if not PDF_SUPPORT:
            raise ImportError("PyMuPDF not installed")

        print(f"Converting PDF to images (DPI={dpi})...")
        os.makedirs(output_folder, exist_ok=True)

        try:
            doc = fitz.open(pdf_path)
            image_paths = []
            for page_num in range(len(doc)):
                page = doc[page_num]
                zoom = dpi / 72
                mat = fitz.Matrix(zoom, zoom)
                pix = page.get_pixmap(matrix=mat, alpha=False)
                output_path = os.path.join(output_folder, f'page_{page_num+1}.png')
                pix.save(output_path)
                image_paths.append(output_path)
                print(f"  ‚úì Page {page_num+1}/{len(doc)} saved: {output_path}")
            doc.close()
            return image_paths
        except Exception as e:
            print(f"‚úó Error converting PDF: {e}")
            return []

    # ============================================================
    # üîπ Preprocess ‡∏†‡∏≤‡∏û‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥
    # ============================================================
    def preprocess_for_low_quality(self, image_path):
        img = cv2.imread(image_path)
        if img is None:
            raise ValueError(f"Cannot read image: {image_path}")

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        height, width = gray.shape
        if height < 3500:
            scale = 3500 / height
            gray = cv2.resize(gray, (int(width * scale), int(height * scale)), interpolation=cv2.INTER_LANCZOS4)

        bilateral = cv2.bilateralFilter(gray, 9, 75, 75)
        denoised = cv2.fastNlMeansDenoising(bilateral, None, 20, 7, 21)
        kernel_sharpen = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])
        sharpened = cv2.filter2D(denoised, -1, kernel_sharpen)
        clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(sharpened)
        gaussian = cv2.GaussianBlur(enhanced, (0, 0), 2.0)
        unsharp = cv2.addWeighted(enhanced, 1.5, gaussian, -0.5, 0)
        binary = cv2.adaptiveThreshold(unsharp, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                       cv2.THRESH_BINARY, 21, 2)
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        return binary

    # ============================================================
    # üîπ Preprocess ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û PDF ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
    # ============================================================
    def preprocess_for_high_quality(self, image_path):
        img = cv2.imread(image_path)
        if img is None:
            raise ValueError(f"Cannot read image: {image_path}")

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        height, width = gray.shape
        if height < 2500:
            scale = 2500 / height
            gray = cv2.resize(gray, (int(width * scale), int(height * scale)), interpolation=cv2.INTER_CUBIC)
        denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)
        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))
        enhanced = clahe.apply(denoised)
        binary = cv2.adaptiveThreshold(enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                       cv2.THRESH_BINARY, 15, 4)
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 1))
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        return binary

    # ============================================================
    # üîπ OCR ‡∏´‡∏•‡∏≤‡∏¢ config ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    # ============================================================
    def ocr_with_multiple_configs(self, image):
        configs = [
            '--oem 1 --psm 6 -l tha',
            '--oem 1 --psm 4 -l tha',
            '--oem 1 --psm 3 -l tha',
            '--oem 3 --psm 6 -l tha',
            '--oem 1 --psm 11 -l tha',
        ]

        results = []
        for config in configs:
            try:
                text = pytesseract.image_to_string(image, config=config)
                data = pytesseract.image_to_data(image, config=config, output_type=pytesseract.Output.DICT)
                confs = [int(c) for c in data['conf'] if str(c) != '-1']
                avg_conf = sum(confs) / len(confs) if confs else 0
                thai_chars = len(re.findall(r'[‡∏Å-‡∏Æ‡∏∞-‡πå]', text))
                results.append({'text': text, 'confidence': avg_conf, 'thai_chars': thai_chars, 'config': config})
            except Exception:
                pass

        if results:
            return max(results, key=lambda x: x['confidence'] * 0.7 + x['thai_chars'] * 0.3)
        return None

    # ============================================================
    # üîπ Post-processing ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏ö‡πà‡∏≠‡∏¢
    # ============================================================
    def post_process_thai_document(self, text):
        result = text
        common_errors = {
            '‡∏Ñ‡∏ó‡∏∞': '‡∏Ñ‡∏ì‡∏∞', '‡∏ó‡∏ó': '‡∏ô‡∏±‡∏Å', '‡∏Å‡∏ó': '‡∏Å‡∏≥', '‡∏™‡∏≥‡∏ó‡∏ó': '‡∏™‡∏≥‡∏ô‡∏±‡∏Å',
            '‡πÄ‡∏£‡∏ó‡∏≠‡∏á': '‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡∏ß‡∏ó‡∏ó‡∏µ‡πà': '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ': '‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà',
            '‡∏ñ‡∏ó': '‡∏ñ‡∏∂‡∏á', '‡∏≠‡∏ó‡∏á‡∏ñ‡∏ó': '‡∏≠‡πâ‡∏≤‡∏á‡∏ñ‡∏∂‡∏á', '‡∏ó‡∏≤‡∏Å‡∏≤‡∏£': '‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',
            '‡∏Å‡∏≤‡∏´‡∏ô‡∏î': '‡∏Å‡∏≥‡∏´‡∏ô‡∏î', '‡∏î‡∏≤‡πÄ‡∏ô‡∏¥‡∏ô': '‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô', '‡∏™‡∏≤‡∏Ñ‡∏±‡∏ç': '‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç',
            '‡∏™‡∏≤‡∏´‡∏£‡∏±‡∏ö': '‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö', '‡∏Ñ‡∏≤‡∏ô‡∏≥': '‡∏Ñ‡∏≥‡∏ô‡∏≥', '‡∏Ñ‡∏≤‡∏™‡∏±‡πà‡∏á': '‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á',
            '‡∏Ñ‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥': '‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥', '‡∏Ñ‡∏≤‡∏Ç‡∏≠': '‡∏Ñ‡∏≥‡∏Ç‡∏≠', '‡∏Ñ‡∏≤‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á': '‡∏Ñ‡∏≥‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á',
            '‡∏Ñ‡∏≤‡∏£‡∏∞‡∏ö‡∏∏': '‡∏Ñ‡∏≥‡∏£‡∏∞‡∏ö‡∏∏', '‡∏ô‡∏≤‡πÄ‡∏™‡∏ô‡∏≠': '‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠', '‡∏î‡∏≤‡∏£‡∏á': '‡∏î‡∏≥‡∏£‡∏á',
            '‡∏Ñ‡∏≤‡∏£‡πâ‡∏≠‡∏á': '‡∏Ñ‡∏≥‡∏£‡πâ‡∏≠‡∏á', '‡∏Ñ‡∏≤‡∏ï‡∏≠‡∏ö': '‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö', '‡∏Ñ‡∏≤‡∏û‡∏¥‡∏û‡∏≤‡∏Å‡∏©‡∏≤': '‡∏Ñ‡∏≥‡∏û‡∏¥‡∏û‡∏≤‡∏Å‡∏©‡∏≤',
            '‡∏™‡∏≤‡πÄ‡∏ô‡∏≤': '‡∏™‡∏≥‡πÄ‡∏ô‡∏≤', '‡∏™‡∏≤‡πÄ‡∏£‡πá‡∏à': '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏Ñ‡∏≤‡πÅ‡∏ñ‡∏•‡∏á': '‡∏Ñ‡∏≥‡πÅ‡∏ñ‡∏•‡∏á',
            '‡∏Ñ‡∏≤‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢': '‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢', '‡∏Ñ‡∏≤‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô': '‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô', '‡∏ó‡∏≤‡∏á‡∏≤‡∏ô': '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô',
            '‡∏Ñ‡∏≤‡πÄ‡∏™‡∏ô‡∏≠': '‡∏Ñ‡∏≥‡πÄ‡∏™‡∏ô‡∏≠', '‡∏Ñ‡∏≤‡πÅ‡∏õ‡∏•': '‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•',
        }
        for wrong, correct in common_errors.items():
            result = result.replace(wrong, correct)

        # ‡∏•‡πâ‡∏≤‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô
        result = re.sub(r'([‡∏Å-‡∏Æ])\s+([‡∏∞-‡∏π])', r'\1\2', result)
        result = re.sub(r'([‡∏±-‡∏π])\s+([‡∏Å-‡∏Æ])', r'\1\2', result)
        result = re.sub(r' +', ' ', result)
        result = re.sub(r'\n\s*\n\s*\n+', '\n\n', result)
        return result.strip()

    # ============================================================
    # üîπ ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡πÉ‡∏ä‡πâ PyThaiNLP)
    # ============================================================
    def correct_thai_spelling(self, text):
        try:
            from pythainlp import spell
            from pythainlp.tokenize import word_tokenize
        except ImportError:
            print("‚ö†Ô∏è ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyThaiNLP ‡∏Å‡πà‡∏≠‡∏ô: pip install pythainlp")
            return text

        words = word_tokenize(text, engine="newmm")
        corrected = []
        for w in words:
            if not re.match(r'^[‡∏Å-‡πô]+$', w):
                corrected.append(w)
                continue
            suggestion = spell(w)
            if suggestion and suggestion[0] != w:
                corrected.append(suggestion[0])
            else:
                corrected.append(w)
        return ''.join(corrected).strip()





    def extract_key_fields(self, text):
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        """
        fields = {}
    
        # ============================================================
        # üîπ ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ ‚Äî ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö (‡∏°‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà")
        # ============================================================
        match = re.search(
            r'(?:‡∏ö\s*)?(?:‡πÄ[‡∏≠‡∏ô][‡πá‡∏ô]‡∏ó‡∏µ|‡πÄ‡∏≠‡πá‡∏ô‡∏ó‡∏µ|‡∏®‡∏ò|‡∏ô‡∏ó|‡∏á‡∏õ|‡∏Ñ‡∏™|‡∏ô‡∏û|‡∏ú‡∏™)\S*\/\S*(?:\s*‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\s*\d{1,2}\s*[‡∏Å-‡πô]+\s*\d{4})?',
            text
        )
        if match:
            number_text = match.group(0).strip()
            number_text = re.sub(r'^‡∏ö\s*', '', number_text)  # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏ö" ‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏≠‡∏Å
    
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° "‡πÄ‡∏≠‡πá‡∏ô‡∏ó‡∏µ" ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
            if not number_text.startswith("‡πÄ‡∏≠‡πá‡∏ô‡∏ó‡∏µ"):
                number_text = re.sub(r'^(‡πÄ[‡∏≠‡∏ô][‡πá‡∏ô]‡∏ó‡∏µ)', '‡πÄ‡∏≠‡πá‡∏ô‡∏ó‡∏µ', number_text)  # normalize ‡∏ï‡∏±‡∏ß‡∏™‡∏∞‡∏Å‡∏î
                if not number_text.startswith("‡πÄ‡∏≠‡πá‡∏ô‡∏ó‡∏µ"):
                    number_text = f"‡πÄ‡∏≠‡πá‡∏ô‡∏ó‡∏µ{number_text}"
    
            fields["‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠"] = number_text
    
        else:
            # fallback: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà"
            match = re.search(r'‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà[:\s]*([^\n]+)', text)
            if match:
                num = match.group(1).strip()
                if not num.startswith("‡πÄ‡∏≠‡πá‡∏ô‡∏ó‡∏µ"):
                    num = f"‡πÄ‡∏≠‡πá‡∏ô‡∏ó‡∏µ{num}"
                fields['‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠'] = num
    
        # ============================================================
        # üîπ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠
        # ============================================================
        match = re.search(r'‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà[:\s]*([^\n]+)', text)
        if match:
            fields['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠'] = match.group(1).strip()
    
        # ============================================================
        # üîπ ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
        # ============================================================
        match = re.search(r'‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á[:\s]*([^\n]+(?:\n(?!\s*‡πÄ‡∏£‡∏µ‡∏¢‡∏ô)[^\n]+)*)', text)
        if match:
            fields['‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á'] = match.group(1).strip()
    
        # ============================================================
        # üîπ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô / ‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö
        # ============================================================
        match = re.search(r'‡πÄ‡∏£‡∏µ‡∏¢‡∏ô[:\s]*([^\n]+)', text)
        if match:
            fields['‡πÄ‡∏£‡∏µ‡∏¢‡∏ô'] = match.group(1).strip()
    
        # ============================================================
        # üîπ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (3‚Äì5 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‚Äú‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‚Äù)
        # ============================================================
        body_match = re.search(r'‡πÄ‡∏£‡∏µ‡∏¢‡∏ô[:\s]*[^\n]+\n(.*)', text, re.DOTALL)
        if body_match:
            body_lines = body_match.group(1).strip().splitlines()
            preview = "\n".join(body_lines[:5])  # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 5 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å
            preview = re.sub(r'\s{2,}', ' ', preview)  # ‡∏•‡πâ‡∏≤‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥
            fields['‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤'] = preview.strip()
    
        return fields
 

 

    # ============================================================
    # üîπ Pipeline ‡∏´‡∏•‡∏±‡∏Å
    # ============================================================
    def process_document(self, file_path, save_debug=True, from_pdf=False):
        print(f"\n{'='*60}\nProcessing: {file_path}\n{'='*60}\n")
        is_pdf = file_path.lower().endswith('.pdf')

        # üî∏ PDF mode
        if is_pdf and PDF_SUPPORT:
            if self.check_pdf_has_text(file_path):
                text = self.extract_text_from_pdf(file_path)
                if text:
                    cleaned = self.correct_thai_spelling(self.post_process_thai_document(text))
                    return {
                        'text': cleaned,
                        'key_fields': self.extract_key_fields(cleaned),
                        'method': 'PDF text layer',
                        'confidence': 100.0
                    }

            # üî∏ ‡πÑ‡∏°‡πà‡∏°‡∏µ text layer
            print("‚úì No text layer found - converting to images for OCR...")
            imgs = self.pdf_to_images(file_path, dpi=300)
            results = [self._process_image(img, save_debug, True) for img in imgs if img]
            valid = [r for r in results if r]
            if not valid: return None
            combined = '\n\n--- ‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà ---\n\n'.join(r['text'] for r in valid)
            avg_conf = sum(r['confidence'] for r in valid) / len(valid)
            cleaned = self.post_process_thai_document(combined)
            return {
                'text': cleaned,
                'key_fields': self.extract_key_fields(cleaned),
                'method': 'PDF OCR',
                'confidence': avg_conf,
                'pages': len(valid)
            }

        # üîπ Image mode
        return self._process_image(file_path, save_debug, from_pdf)

    # ============================================================
    # üîπ Process ‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
    # ============================================================
    def _process_image(self, image_path, save_debug, high_quality=False):
        processed = self.preprocess_for_high_quality(image_path) if high_quality else self.preprocess_for_low_quality(image_path)
        if save_debug:
            cv2.imwrite(image_path.replace('.', '_debug.'), processed)
        result = self.ocr_with_multiple_configs(processed)
        if not result: return None
        cleaned = self.correct_thai_spelling(self.post_process_thai_document(result['text']))
        return {
            'text': cleaned,
            'key_fields': self.extract_key_fields(cleaned),
            'confidence': result['confidence']
        }


# ============================================================
# üî∏ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö standalone
# ============================================================
if __name__ == "__main__":
    ocr = EnhancedThaiDocumentOCR()
    test_file = "document.pdf"
    try:
        result = ocr.process_document(test_file, save_debug=True)
        if result:
            print("\n=== OCR SUCCESS ===")
            print(f"Method: {result.get('method')}")
            print(f"Confidence: {result['confidence']:.2f}%")
            print("Key Fields:", result['key_fields'])
            print("\nText:\n", result['text'][:800], "...")
    except Exception as e:
        print(f"Error: {e}")





